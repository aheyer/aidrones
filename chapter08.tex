\renewcommand{\deutschertitel}{Stereokameras mit einem Raspberry Pi 5}
\renewcommand{\englischertitel}{Stereo camera setup with a Raspberry Pi 5}
\chapter[\texorpdfstring{\protect{\vspace{2pt}\englischertitel}}{\englischertitel}]{}
\kapitel{\deutschertitel}
\thispagestyle{empty}

\label{KapitelStereoVision}

\begin{paracol}{2}[]

{\raggedright\huge\bfseries\sffamily \englischertitel \par\ } \\[1.8ex]

\switchcolumn

{\raggedright\huge\bfseries\sffamily \deutschertitel \par\ } \\[1.8ex]

\coleng

Test Eng

\colger

Stereokameras sind ein klassisches Werkzeug, um Tiefenerkennung durchzuführen. Der Vorteil gegenüber einem einfachen Kamerasetup, ist das kein Modell trainiert werden muss, um die Tiefenerkennung zu konfigurieren. 

Alles was dafür benötigt wird sind zwei baugleiche Kameras und ein Computer, der die entsprechenden Berechnungen mithilfe von OpenCV durchführt. Alternativ kann auch ein Stereokameramodul verwendet werden, welches explizit für solche Anwendungsfälle gedacht ist. Diese sind meistens platzsparender und einfacher zu montieren/anzuschließen. Allerdings verursachen sie zusätzliche Kosten.

Der folgende Guide basiert auf einem Raspberry Pi 5 mit 8 GB Arbeitsspeicher und zwei Kameramodulen vom Modell: RB-CAMERA-JT-V2-120. Der Grund hierfür ist, dass nur das Raspberry Pi 5 zwei Kameramodule mit paralleler Nutzung ermöglicht. Vorangegangene Modelle würden einen Adapter benötigen, welcher keine parallele Nutzung beider Kameras zulässt.

\colende

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\linewidth]{StereoVision_Contraption.jpg}
	\caption{Stereo Vision Contraption}
	\label{StereoVisionContraption}
\end{figure}

\colstart

Test Eng

\colger

Die Kameras werden nebeneinander montiert, und mit zwei Minikabeln angeschlossen. Die Kameras sollten möglichst parallel montiert werden, da es jedoch nicht möglich ist dies perfekt zu machen wird es später softwareseitig korrigiert.

Ein solches Setup ist jedoch nur für größere Drohnen geeignet, welche große Batterikapazitäten tragen können. Bei kleineren Drohnen würde das Raspberry Pi 5 die Flugzeit sehr stark reduzieren würde. Für kleinere Drohnen, sollte mit einem einfachen Kamerasetup und einem Modell gearbeitet werden.

\colende

\renewcommand{\deutschertitel}{Camera Calibration}
\renewcommand{\englischertitel}{Kamerakalibrierung}
\makroabschnitt
\label{AbschnittStereoVisionKalibrierung}

Test Eng

\colger

Die Notwendigkeit der Kamerakalibrierung ergibt sich aus der Konstruktion (Abbildung~\ref{StereoVisionContraption}), denn die beiden Kameras sind nicht exakt parallel montiert. Es wäre theoretisch möglich diese Konstruktion immer wieder iterativ anzupassen bis dies der Fall ist, jedoch ist es äußerst unwahrscheinlich, dass die Linsen der beiden Kameras exakt gleich sind.

Das ist vor allem relevant für die Raspberry-Pi-Kameras, da es sich hier um sogenannte Pinhole-Kameras handelt. Solche Kameras haben häufig eine recht starke Radialverzerrung. Diese Art von Verzerrung lässt gerade Linien kurvig erscheinen. Eine weitere relevante Art der Verzerrung ist die Tangentialverzerrung. Diese entsteht, wenn die Linse der Kamera nicht komplett parallel zur Oberfläche ist, die sie aufnimmt. Dies ist ebenfalls äußerst schwierig zu bewerkstelligen, besonders mit zwei Kameras.

Das bedeutet also, dass unabhängig von den verwendeten Kameramodulen (inkl. Stereomodulen) eine Kalibrierung notwenig ist. Dabei werden die beiden Kameras zunächst einzeln kalibriert und dann zusammen einer Stereokalibrierung unterzogen.

Bei der Kalibrierung wird versucht diese sogenannten Verzerrungskoeffizienten zu finden.
Diese werden durch $k_1$, $k_2$, $k_3$ für die Radialverzerrung und $p_1$ und $p_2$ für die Tangentialverzerrung.

$$ dist = (k_1, k_2, k_3, p_1, p_2) $$

Darüber hinaus werden intrinsischen und extrinsischen Kameraparameter berechnet. Die intrinsischen Parameter beziehen sich auf die Einstellungen der Kamera. Dabei handelt es sich um die Brennweite $(f_x, f_y)$ und das optische Zentrum $(c_x, c_y)$. Aus diesen beiden Pixelwerten ergibt sich die Kameramatrix.

$$ camMtx = \begin{bmatrix}
	f_x & 0 & c_x \\
	0 & f_y & c_y \\
	0 & 0 & 1
\end{bmatrix}$$

Bei den extrinsischen Parametern handelt es sich um die Rotationsmatrix und den Übersetzungsvektor ($R$ und $t$), welche benötigt werden, um den 3D-Punkt in der physischen Welt auf den 2D-Punkt in der Abbildung zu projizieren. Diese können dann zu einer homogenen Transformation kombiniert werden, welche die Übersetzung des Punktes in der realen Welt ($P_w$) zu einem Punkt im Koordinatensystem der Kamera ($P_c$) beschreibt.

$$ P_c = \begin{bmatrix}
	R & t \\
	0 & 1
\end{bmatrix} P_w$$ 

Mithilfe dieser Parameter ist es möglich ein Bild, was mit einer kalibrierten Kamera aufgenommen wurde zu entzerren. Dies ist ein Zwischenschritt bei der Aufnahmen von Stereobildern. Wichtig ist dabei, dass das Bild mit der gleichen Auflösung aufgenommen werden muss, mit der die Kamera kalibriert wurde. Sollte das Bild skaliert werden, müssen zumindest die intrinsischen Kameraparameter um den selben Faktor skaliert werden. 

Die Kalibrierung erfolgt mithilfe eines Kalibrierungsmusters. Hierbei besteht die Auswahl zwischen einem Schachbrettmuster, einem Raster aus Kreisen und einem sogennanten ChArUco-Board. Letzteres ist ein Schachbrettmuster, welches mit Aruco-Markierungen versehen wurde.

\colende

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\linewidth]{Stereo_Vision_Charuco.png}
	\caption{ChArUco Board Example}
	\label{ImageStereoVisionCharucoBoard}
	% Source: https://docs.opencv.org/4.x/df/d4a/tutorial_charuco_detection.html
\end{figure}

\colstart

Eng

\colger

Der Vorteil dieses ChArUco-Patterns ist, dass die Aruco-Markierungen einzigartig sind und individuell zugeordnet werden können. Dies ermöglicht es Kalibrierungsbilder aufzunehmen, die teilweise bedeckt sind oder auf denen nicht alle Ecken des Schachbretts zu sehen sind. Dies verspricht in der Regel eine höhere Genauigkeit und sollte, wenn möglich einem Schachbrettmuster vorgezogen werden.

Bei den anderen beiden Pattern müssen alle Ecken, bzw. Kreise im Bild zu sehen sein. Dies macht es besonders schwierig die Ecken abzudecken, welche oft kritisch sind für eine gute Genauigkeit. Des weiteren werden dabei mehr Bilder benötigt. Wie viele Bilder konkret benötigt werden, sollte im Einzelfall getestet werden. Zehn sind grundsätzlich ein guter Startpunkt, da diese mindestens für die Kalibrierung mit einem Schachbrettmuster benötigt werden. Die anderen Muster können gegebenenfalls auch mit weniger auskommen. 

Unabhängig vom gewählten Kalibrierungsmuster, sollte die Kalibrierung ungefähr in der Entfernung durchgeführt werden, in der die Kamera später eingesetzt wird. Darüber hinaus sollte das Muster mindestens die Hälfte des Bildes bedecken. Außerdem sollte es auf einer flachen Oberfläche abgebildet werden und hochauflösend sein. Es empfiehlt sich hier einen Bildschirm zu verwenden.

Die Bilder sollten auch aus unterschiedlichen Winkeln und mit unterschiedlichen Neigungen gemacht werden. Auf eine gute Belichtung sollte auch geachtet werden, da ungleichmäßige Belichtung dafür sorgen kann, dass nicht alle Punkte auf jedem Bild erkannt werden. 

Für das Aufnehmen der Bilder in einem Dual-Camera-Setup empfiehlt es sich den Auslöser zu synchronisieren. Im Falle von Python kann dazu \texttt{threading.Barrier()} verwendet werden (siehe Abbildung~\ref{ListingTakingCalibrationPictures}). 

\colende

\begin{figure}
	\begin{lstlisting}[style=shell]
from picamera2 import Picamera2
from time import sleep
import threading
from libcamera import Transform

cam_zero = Picamera2(0)
cam_one = Picamera2(1)
barrier = threading.Barrier(2)

def capture_zero(index):
	barrier.wait()
	cam_zero.capture_file(f"/path/to/images_0/{index}.jpg")

def capture_one(index):
	barrier.wait()
	cam_one.capture_file(f"/path/to/images_1/{index}.jpg")

try:
	while True:
		print(counter)
		pic_zero = threading.Thread(target=capture_zero, args=(counter,))
		pic_one = threading.Thread(target=capture_one, args=(counter,))
		pic_zero.start()
		pic_one.start()
		pic_zero.join()
		pic_one.join()
		counter += 1
		sleep(3)

except KeyboardInterrupt:
	cam_zero.close()
	cam_one.close()
	\end{lstlisting}
	\caption{Test Listing}
	\label{ListingTakingCalibrationPictures}
\end{figure}

\colstart

Test Eng

\colger

Als letzter Schritt vor der Kalibrierung müssen die Bilder der beiden Kameras ausgewertet werden. Dies geschieht mit Hilfe von \texttt{cv2.aruco.CharucoDetector(board)}. Dieser wird verwendet, um die Ecken des Schachbretts und die entsprechnden Aruco-Markierungen zu erkennen und als Zahlenwerte zu beschreiben. Diese Zahlenwerte werden dann als Input für \texttt{cv2.calibrateCamera()} verwendet.

Der Output dieser Funktion sind die intrinsischen und extrinsischen Kameraparameter die Verzerrungskoeffizenten, die Rotationsmatrix und der Übersetzungsvektor, sowie die mittlere quadratische Abweichung zur Evaluation der Kalibrierung. Diese sollte idealerweise kleiner als 1 sein. Das ist jedoch noch keine Garantie für ein gutes Resultat. Sollte die Abweichung kleiner als 1 sein und die Bilder können nicht entzerrt werden, empfiehlt es sich die Abweichung der Bilder einzeln zu prüfen. 

Nicht immer erkennt der Detector alle relevanten Punkte, unabhängig von gutem Licht. Durch das löschen schlechter Bilder kann das Resultat weiter verbessert werden. Zu diesem Zweck können die mit \texttt{detector.detectBoard(image)} entdeckten Marker mit \texttt{cv2.aruco.drawDetectedCornersCharuco()} gezeichnet werden und dann mit \texttt{cv2.imwrite()} ausgegeben werden. 

Durch visuelles inspizieren können Probleme mit den Kalibrierungsbildern einfacher erkannt werden. Dies ist vor Allem interessant für Schachbrettmuster, da nicht garantiert ist, dass die Ecken immer in der gleichen Reihenfolge erkannt werden. 

\colende

\begin{figure}
	\begin{lstlisting}[style=shell]
import cv2
import numpy as np
import os
from glob import glob

def charuco_calibration(images_glob : str, cam : int):
	dictionary = cv2.aruco.getPredefinedDictionary(ARUCO_DICT)
	board = cv2.aruco.CharucoBoard((SQUARES_VERTICALLY, SQUARES_HORIZONTALLY), SQUARE_LENGTH, MARKER_LENGTH, dictionary)
	params = cv2.aruco.DetectorParameters()
	detector = cv2.aruco.CharucoDetector(board)
	image_paths = glob(images_glob)
	all_charuco_corners = []
	all_charuco_ids = []
	all_image_pts = []
	all_object_pts = []
	counter = 1
	
	for image_path in image_paths:
		image = cv2.imread(image_path)
		grayscaled = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
		charuco_corners, charuco_ids, marker_corners, marker_ids = detector.detectBoard(image)
		
		if charuco_corners is not None:   
			image = cv2.aruco.drawDetectedCornersCharuco(image, charuco_corners, charuco_ids)
			object_points, image_points = board.matchImagePoints(charuco_corners, charuco_ids)
			all_object_pts.append(object_points)
			all_image_pts.append(image_points)
			all_charuco_corners.append(charuco_corners)
			all_charuco_ids.append(charuco_ids)

			counter += 1
			
	image_height, image_width = image.shape[:2]

	retval, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(all_object_pts, all_image_pts, (image_height, image_width), None, None)
	return all_object_pts, all_image_pts, retval, camera_matrix, dist_coeffs, rvecs, tvecs, image_height, image_width
	\end{lstlisting}
	\caption{Single Camera calibration}
	\label{ListingCameraCalibration}
\end{figure}

\colstart

Test Eng

\colger

Nach der Kalibrierung kann die intrinsische Kameramatrix mit \texttt{cv2.getOptimalCameraMatrix()} verfeinert werden. Der dazu verwendete Skalierungsfaktor $\alpha = [0, 1] $ bestimmt wie viele ungewollte Pixel im Bild übrig bleiben. Wenn $\alpha = 1$ werden alle Pixel des ursprünglichen Bilds behalten. Dadurch entstehen jedoch einige Bereiche mit schwarzen Pixeln. Wenn $\alpha = 0$, dann werden diese Bereiche auf ein minimum reduziert, dadurch wird jedoch unter Umständen nicht die Vollständige Information des ursprünglichen Bilds bewahrt.

Zum Testen kann die Funktion \texttt{cv2.undistort()} verwendet werden. Mit \texttt{cv2.imwrite()} kann das Bild dann ausgegeben werden. Sollten zu viele schwarze Pixel vorhanden sein und $\alpha = 0$ kann das Bild zusätzlich auf die \textsl{Region-of-Interest (ROI)} verkleinert werden. 

\colende

\renewcommand{\deutschertitel}{Stereo-Kalibrierung}
\renewcommand{\englischertitel}{Stereo Calibration}
\makroabschnitt
\label{AbschnittStereoKalibrierung}

Test Eng

\colger

Nach einer erfolgreichen Kalibrierung der einzelnen Kameras ist es jeweils möglich einen 3D-Punkt auf ein 2D-Bild zu projizieren und damit zu entzerren. Um die 3D-Information wiederherzustellen muss berechnet werden, wie die beiden Kameras zueinanderstehen. 

Hierfür wird die Funktion \texttt{cv2.stereoCalibrate} verwendet (Abbildung \ref{ListingStereoCalibration}). Die Variable \texttt{criteria\_stereo} ermöglicht es die die Berechnung zu verfeinern. Die Flagge \texttt{cv2.CALIB\_FIX\_INTRINSIC\_GUESS} sorgt dafür, dass die intrinsischen Matrizen aus der einzelnen Kalibrierung nicht verändert werden, während dieses Prozesses.

Wichtig ist dabei, dass die Kalibrierungsbilder synchronisiert sind. Dies wird mit dem Code in Abbildung \ref{ListingTakingCalibrationPictures} sichergestellt. Bei Bedarf kann allerdings auch ein zweiter Satz Kalibrierungsbilder angefertigt werden. In diesen müssen dann die Punkte erneut entdeckt werden. 

\colende

\begin{figure}
	\begin{lstlisting}[style=shell]
criteria_stereo = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.0001)
retS, new_mtx_zero, dist_zero, new_mtx_one, dist_one, Rot, Trns, Emat, Fmat = cv2.stereoCalibrate(obj_pts_one, img_pts_zero, img_pts_one, mtx_zero, dist_zero, mtx_one, dist_one, (w_zero,h_zero), criteria_stereo, cv2.CALIB_FIX_INTRINSIC_GUESS)
	\end{lstlisting}
	\caption{Single Camera calibration}
	\label{ListingStereoCalibration}
\end{figure}

\colstart

Test Eng

\colger

Das Ergebnis der Stereokalibrierung sind die Rotations- und Übersetzungsvektoren $R \text{und} t$ zwischen den beiden Kameras, sowie die essentielle ($E$) und fundamentale Matrix ($F$). Die Rotationsmatrix beschreibt die 3D-Rotation von Kamera 1 zu Kamera 2. Die Übersetzungsmatrix beschreibt die Verschiebung des Koordinatensystems der ersten Kamera gegenüber der zweiten Kamera.

Die essentielle Matrix ergibt sich aus den beiden vorangegangenen Vektoren ($E = [t] \times R $). Mit dieser Matrix lässt sich bestimmen wo ein Punkt auf dem Koordinatensystem von Kamera 1 auf dem von Kamera 2 liegt. Die Linie zwischen diesen beiden Punkten wird als \textsl{epipolare Linie} bezeichnet. 

Die fundamentale Matrix erfüllt den gleichen Zweck, jedoch mit Pixelwerten, anstatt normalisierter Koordinaten. Sie wird mithilfe der essentiellen Matrix und den beiden intrinsischen Matrizen der Kamera berechnet.

$$ F = K'^{-\top} E K^{-1} $$ 

Mithilfe dieser Matrizen kann die Funktion \texttt{cv2.stereoRectify()} angewendet werden, um die epipolaren Linien horizontal zu machen. Dadurch haben beide Bilder die gleichen Y-Koordinaten, auch wenn sie nicht perfekt horizontal montiert sind und die Tiefenerkennung wird deutlich robuster. Sie wird robuster, da alle Punkte in Bild 1 auch in Bild 2 gefunden werden können. Die Resultate beinhalten die Korrektur- und Projektionsmatrizen, sowie die Matrix $Q$ und die jeweilige ROI der beiden Bilder.

% Read over Kamerakalibrierung and correct projection from 3D to 2D.

In diesem Fall beschreibt die ROI einen Bereich im Bild, in welchem alle Pixel valide sind. Die Korrekturmatrizen beschreiben, wie die jeweiligen Bilder angepasst werden müssen, um horizontale Epipolarlinien zu erhalten. Die Projektionsmatrizen ($P_0, P_1$) beschreiben die notwendige Transformation, um einen 3D-Punkt auf das jeweilige 2D-Koordinatensystem zu projizieren. Diese erweitern die ursprüngliche intrinsische Kameramatrix $camMtx$ um eine Spalte:

$$ P_0, P_1 = \begin{bmatrix}
	f_x & 0 & c_x & t_x \\
	0 & f_y & c_y & 0 \\
	0 & 0 & 1 & 0
\end{bmatrix}$$

Der hinzugefügte Wert $t_x$ beschreibt den Horizontalen Versatz zwischen den beiden Bildern als Pixelwert. $f$ und $c$ beschreiben weiterhin die Brennweite und das optische Zentrum. Normalerweise sollte nach Verwendung von \texttt{cv2.stereoRectify()} $f_x = f_y$ gelten.

Die Matrix $Q$ beschreibt eine \textsl{Disparity-to-Depth-Matrix}. Wie diese verwendet werden kann, wird in Abschnitt \ref{AbschnittStereoVisionTiefenerkennung} erläutert.

\colende

\begin{figure}
	\begin{lstlisting}[style=shell]
rectify_scale = 0
rect_zero, rect_one, proj_mat_zero, proj_mat_one, Q, roi_zero, roi_one = cv2.stereoRectify(new_mtx_zero, dist_zero, new_mtx_one, dist_one, (w_zero,h_zero), Rot, Trns, rectify_scale,(0,0))
	\end{lstlisting}
	\caption{Stereo Rectification}
	\label{ListingStereoRectification}
\end{figure}

\colstart

Test Eng

\colger

Da die beiden Kameras statisch montiert sind, genügt es die Rectification einmal zu berechnen und dann zu speichern. Dadurch kann viel Rechenzeit gespart werden, wodurch die Performance verbessert wird. Dazu kann die Funktion \texttt{cv2.initUndistortRectifyMap()} verwendet werden.
Diese erstellt eine Mapping-Funktion zwischen den Kameras die immer wieder verwendet werden kann. 

\colende

\begin{figure}
	\begin{lstlisting}[style=shell]
Stereo_Map_Zero = cv2.initUndistortRectifyMap(new_mtx_zero, dist_zero, rect_zero, proj_mat_zero, (w_zero,h_zero), cv2.CV_16SC2)
Stereo_Map_One = cv2.initUndistortRectifyMap(new_mtx_one, dist_one, rect_one, proj_mat_one, (w_zero,h_zero), cv2.CV_16SC2)

cv_file = cv2.FileStorage("/path/to/stereo_maps.xml", cv2.FILE_STORAGE_WRITE)
cv_file.write("Stereo_Map_Zero_x",Stereo_Map_Zero[0])
cv_file.write("Stereo_Map_Zero_y",Stereo_Map_Zero[1])
cv_file.write("Stereo_Map_One_x",Stereo_Map_One[0])
cv_file.write("Stereo_Map_One_y",Stereo_Map_One[1])
cv_file.release()
	\end{lstlisting}
	\caption{Initializing the stereo mapping function}
	\label{ListingInitUndistortRectifyMap}
\end{figure}

\renewcommand{\deutschertitel}{Depth Estimation}
\renewcommand{\englischertitel}{Tiefenerkennung}
\makroabschnitt
\label{AbschnittStereoVisionTiefenerkennung}

Test Eng

\colger

Nun da die Bilder auf eine Y-Achse gebracht werden können gilt es die Unterschiede zwischen den aufgenommenen Bildern zu berechnen. Hierzu wird der Block-Matching (BM)-Algorithmus verwendet. Dieser teilt das Bild in kleinere Blöcke (z.B. 5x5 Pixel) auf und sucht nach korrespondierenden Blöcken entlang der Epipolaren Linie im Referenzbild.

Anhand unterschiedlicher Metriken wie die Sum of Absolute Differences (SAD), die Sum of Squared Differences (SSD) und Normalized Cross-Correlation (NCC) werden die Blöcke nacheinander verglichen und eine sogenannte Disparity-Map erstellt werden.

Diese Disparity-Map repräsentiert die Verschiebung zwischen den beiden Bildern (beispielsweise als Grauwert). OpenCV bietet die Klasse \texttt{cv2.StereoBM} an welche verwendet werden kann, um eine solche Disparity-Map zu erstellen.

Alternativ kann auch Semi-Global-Block-Matching (SGBM) verwendet werden, um die Disparity-Map zu erstellen. Hierzu existiert ebenfalls eine OpenCV-Klasse. Diese trägt den Namen \texttt{cv2.StereoSGBM}. SGBM ermöglicht Sub-Pixel-Berechnungen, welche Texturen schärfer erscheinen lassen können.

Sowohl BM als auch SGBM können mit einer Vielzahl an Parametern verfeinert werden. Dies beinhaltet beispielsweise die Blockgröße die geprüft wird, oder ob ein Pre-Filter verwendet werden soll, um Texturen im Bild besser sichtbar zu machen. Die Methode \texttt{compute(img\_, img\_r)}, welche die tatsächliche Berechnung der Disparity durchführt gibt Werte zurück, die mit 16 skaliert wurden. Um die tatsächlichen Disparity-Werte zu erhalten muss das Ergebnis durch 16 geteilt werden.

Aus diesen Disparity-Werten ($D$) kann können die Tiefenwerte ($Z$) berechnet werden. Dazu werden auch die Brennweite ($f$) und die Grundlinie ($B$) benötigt. Die entsprechenden Werte werden nicht aus de ursprünglichen Kalibrierung entnommen, sondern aus den Ergebnissen von \texttt{cv2.stereoCalibrate()} und \texttt{cv2.stereoRectify()}. $B$ ist der erste Wert aus dem Übersetzungsvektor aus der Kalibrierung und die Brennweite kann aus der Projektionsmatrix von Kamera 1 entnommen werden, da nach der Korrektur normalerweise $f_x = f_y$ gilt. Um einen Tiefenwert für einen gegebenen Punkt zu erhalten, muss folgende Berechnung durchgeführt werden.

$$ Z = \frac{Bf}{D} $$

Diese Berechnung muss allerdings nicht als solche durchgeführt werden. Die Matrix $Q$, welche von \texttt{cv2.stereoRectify()} zurückgegeben wurde kann in Verbindung mit der Methode \texttt{reprojectImageTo3D} verwendet werden, um die Tiefenwerte zu erhalten. Diese werden im \texttt{im3d(x, y, ch)} ausgegeben und enthalten die 3D-Koordinaten für einen jeweiligen Punkt $(x, y)$.

Sollte die Berechnung dennoch als solche durchgeführt werden, empfiehlt es sich die Disparity zu normalisieren. Des weiteren muss darauf geachtet werden, dass $D > 0$ für jeden Punkt im Bild, da sonst falsche Tiefenwerte errechnet werden. Dies kann dafür sorgen, dass die resultierenden Tiefenwerte verzerrt wirken.

Ein möglicher Nutzen dieser konkreten Berechnung wäre es, nicht alle Tiefenwerte berechnen zu müssen, sondern nur für interessante Regionen. Dies ließe sich in Verbindung mit einer Objekterkennung realisieren und wäre für höhere Auflösungen interessant. Grundsätzlich ist auf die Limitationen des Raspberry Pi zu achten. Bei einer Auflösung 640x480 ist das Pi in der Lage die Disparity mit einer nutzbaren Framerate zu produzieren. Höhere Auflösungen müssen im Einzelfall geprüft werden.

\colende